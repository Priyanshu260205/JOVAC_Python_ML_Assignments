Assignment Tasks Task 4: Conceptual Questions Answer briefly:

What is entropy and information gain?
Explain the difference between Gini Index and Entropy.
How can a decision tree overfit? How can this be avoided?

1. What is entropy and information gain?

-->Entropy is a measure of impurity or randomness in a dataset. It tells us how mixed the class labels are.
If all the data points belong to the same class, entropy is 0 (pure). Higher entropy means more disorder
and more mixed classes.

-->Information Gain is the amount of reduction in entropy when a dataset is split based on a feature.
It helps identify the best feature for splitting the data. A higher information gain means the feature
gives more useful information for classifying the data.

2. Explain the difference between Gini Index and Entropy.

Both Gini Index and Entropy measure the impurity of a dataset, but they do it differently.
The Gini Index calculates how often a randomly chosen element would be incorrectly classified.
It’s simpler and faster to compute, as it doesn’t use logarithms.

Entropy is based on information theory and measures the level of disorder or uncertainty. It uses 
logarithmic calculations.

In practice, both give similar results, but Gini is usually preferred for speed, while entropy
is preferred when using algorithms like ID3 or C4.5.

3. How can a decision tree overfit? How can this be avoided?

A decision tree overfits when it becomes too complex and learns not only the patterns in the 
training data but also the noise or outliers. This leads to poor performance on new, unseen data.

To avoid overfitting:

-->Use pruning, which removes unnecessary branches from the tree.
-->Set a maximum depth for the tree to prevent it from growing too large.
-->Set a minimum number of samples required to split a node or to be in a leaf.
-->use ensemble methods like Random Forest, which combine multiple decision trees to reduce overfitting.